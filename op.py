import json
import re
import os
from datetime import datetime
from typing import Dict, Optional
import markdown
from markdown.extensions.fenced_code import FencedCodeExtension

class MarkdownToBlogConverter:
    def __init__(self):
        self.md_parser = markdown.Markdown(
            extensions=[
                'extra',
                FencedCodeExtension(),
                'tables',
                'toc',
                'nl2br',
                'sane_lists'
            ],
            output_format='html5'
        )
        
        self.default_metadata = {
            'category': 'æŠ€æœ¯',
            'author': 'æœªçŸ¥ä½œè€…'
        }

    def extract_tag_from_filename(self, filename: str) -> str:
        """ä»æ–‡ä»¶åæå–æ ‡ç­¾ï¼ˆå–æ–‡ä»¶åç¬¬ä¸€éƒ¨åˆ†ï¼‰"""
        # ç§»é™¤æ‰©å±•åå’Œå¸¸è§å‰ç¼€/åç¼€
        name = os.path.splitext(filename)[0]
        name = re.sub(r'^\d+-', '', name)  # ç§»é™¤å¼€å¤´çš„æ•°å­—å’Œç ´æŠ˜å·
        name = re.sub(r'[-_]', ' ', name)  # å°†ä¸‹åˆ’çº¿å’Œä¸­åˆ’çº¿è½¬ä¸ºç©ºæ ¼
        
        # æå–ç¬¬ä¸€ä¸ªæœ‰æ„ä¹‰çš„å•è¯ä½œä¸ºæ ‡ç­¾
        words = name.split()
        if words:
            # ç§»é™¤å¸¸è§åœç”¨è¯
            stop_words = {'how', 'to', 'a', 'an', 'the', 'guide', 'tutorial'}
            for word in words:
                if word.lower() not in stop_words and len(word) > 2:
                    return word.capitalize()
        
        return 'æœªåˆ†ç±»'

    def parse_markdown(self, md_content: str, filename: str) -> Dict:
        """è§£æMarkdownå†…å®¹"""
        metadata, cleaned_content = self.extract_metadata(md_content)
        
        # ä½¿ç”¨æ–‡ä»¶åä½œä¸ºæ ‡é¢˜ï¼ˆä¸å¸¦æ‰©å±•åï¼‰
        title = os.path.splitext(filename)[0].replace('-', ' ').replace('_', ' ')
        
        # ä»æ–‡ä»¶åæå–æ ‡ç­¾
        tag = self.extract_tag_from_filename(filename)
        
        # ç”ŸæˆHTMLå†…å®¹
        html_content = self.md_parser.reset().convert(cleaned_content)
        
        # è‡ªåŠ¨ç”Ÿæˆæ‘˜è¦
        text_content = re.sub(r'<[^>]+>', '', html_content)
        summary = metadata.get('summary') or (text_content[:200] + '...' if len(text_content) > 200 else text_content)
        
        return {
            "id": int(datetime.now().timestamp() % 100000),
            "title": title,
            "date": metadata.get('date') or datetime.now().strftime("%Y-%m-%d"),
            "summary": summary,
            "content": html_content,
            "category": metadata.get('category', self.default_metadata['category']),
            "tag": tag,  # ä»æ–‡ä»¶åæå–çš„å•ä¸ªæ ‡ç­¾
            "author": metadata.get('author', self.default_metadata['author']),
            "word_count": len(text_content.split()),
            "reading_time": max(1, len(text_content.split()) // 200)
        }
    def process_file(self, input_path: str, output_path: Optional[str] = None) -> Optional[Dict]:
        """å¤„ç†å•ä¸ªMarkdownæ–‡ä»¶"""
        try:
            with open(input_path, 'r', encoding='utf-8') as f:
                md_content = f.read()
            
            filename = os.path.basename(input_path)
            post = self.parse_markdown(md_content, filename)
            
            if not output_path:
                output_path = f"{os.path.splitext(input_path)[0]}.json"
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(post, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… è½¬æ¢æˆåŠŸ: {input_path} -> {output_path}")
            return post
        
        except Exception as e:
            print(f"âŒ å¤„ç†æ–‡ä»¶ {input_path} æ—¶å‡ºé”™: {str(e)}")
            return None

    def batch_process(self, input_dir: str, output_dir: Optional[str] = None):
        """æ‰¹é‡å¤„ç†ç›®å½•ä¸­çš„Markdownæ–‡ä»¶"""
        if not output_dir:
            output_dir = input_dir
        
        os.makedirs(output_dir, exist_ok=True)
        all_posts = []
        
        for filename in os.listdir(input_dir):
            if filename.endswith('.md'):
                input_path = os.path.join(input_dir, filename)
                output_path = os.path.join(output_dir, f"{os.path.splitext(filename)[0]}.json")
                post = self.process_file(input_path, output_path)
                if post:
                    all_posts.append(post)
        
        # ç”Ÿæˆç´¢å¼•æ–‡ä»¶
        if all_posts:
            index_path = os.path.join(output_dir, '_index.json')
            with open(index_path, 'w', encoding='utf-8') as f:
                json.dump({
                    "generated_at": datetime.now().isoformat(),
                    "post_count": len(all_posts),
                    "posts": sorted(all_posts, key=lambda x: x['date'], reverse=True)
                }, f, ensure_ascii=False, indent=2)
            print(f"\nğŸ“ ç”Ÿæˆç´¢å¼•æ–‡ä»¶: {index_path}")

if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(
        description='Markdownè½¬JSONåšå®¢å·¥å…·',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    parser.add_argument('input', help='Markdownæ–‡ä»¶è·¯å¾„æˆ–ç›®å½•è·¯å¾„')
    parser.add_argument('-o', '--output', help='è¾“å‡ºè·¯å¾„(æ–‡ä»¶æˆ–ç›®å½•)')
    
    args = parser.parse_args()
    
    converter = MarkdownToBlogConverter()
    
    if os.path.isdir(args.input):
        print(f"ğŸ“‚ æ‰¹é‡å¤„ç†ç›®å½•: {args.input}")
        converter.batch_process(args.input, args.output)
    elif os.path.isfile(args.input):
        print(f"ğŸ“„ å¤„ç†å•ä¸ªæ–‡ä»¶: {args.input}")
        converter.process_file(args.input, args.output)
    else:
        print(f"âŒ é”™è¯¯: è·¯å¾„ {args.input} ä¸å­˜åœ¨")
        exit(1)